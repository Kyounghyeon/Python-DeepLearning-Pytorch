{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 역전파 과정 코딩하기\n",
    "\n",
    "신경망 모형에서는 역전파 과정을 통해 파라미터를 업데이트하여 성능을 높입니다.  \n",
    "파이토치 프레임워크에서는 손실함수를 정의하고 `loss.backward()` 와 같은 메서드로 쉽게 역전파가 가능하지만, 이해가 부족했고 직접 해볼 필요성을 느껴서 파이토치의 텐서를 이용하여 역전파 과정을 짜보고자 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고\n",
    "\n",
    "[모두를 위한 딥러닝 시즌2](https://www.youtube.com/watch?v=B3VG-TeO9Lk&list=PLQ28Nx3M4JrhkqBVIXg-i5_CVVoS1UzAv&index=7)\n",
    "\n",
    "[cs231n Stanford University](https://www.youtube.com/watch?v=i94OvYb6noo&t=989s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 과정\n",
    "1. input layer에 feature의 수만큼 노드를 생성\n",
    "2. N 개의 hidden layer에 직접 노드를 생성\n",
    "3. 각 hidden layer를 통과할 때마다 활성화 함수를 거침\n",
    "4. output layer에 얻고 싶은 결과에 따라 노드를 생성\n",
    "5. chain rule에 의해 gradient 값을 구하고 gradient descent 방법으로 파라미터 업데이트\n",
    "6. 위의 과정 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer \n",
    "X = torch.Tensor([[0,0], [0,1], [1,0], [1,1]]).to(device)\n",
    "y = torch.Tensor([[0], [1], [1], [0]]).to(device)\n",
    "\n",
    "# weight & bias\n",
    "w1 = torch.Tensor([[1,1], [1,1]]).to(device)\n",
    "b1 = torch.Tensor([1,2]).to(device)\n",
    "w2 = torch.Tensor([[2],[1]]).to(device)\n",
    "b2 = torch.Tensor([1]).to(device)\n",
    "\n",
    "# Activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1+torch.exp(-x))\n",
    "\n",
    "# Activation function prime\n",
    "def sigmoid_p(x):\n",
    "    return sigmoid(x) * (1-sigmoid(x))\n",
    "\n",
    "# Loss function (binary cross entropy)\n",
    "def CE(y, y_pred):\n",
    "    return torch.mean(y * torch.log(y_pred) + (1-y) * torch.log(1-y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sigmoid**\n",
    "$$ sigmoid(x) = \\frac{1}{1+e^{-x}}$$  \n",
    "$$ \\frac{d}{dx}sigmoid(x) = \\frac{1}{1+e^{-x}} \\times \\frac{e^{-x}}{1+e^{-x}}$$ \n",
    "\n",
    "**Binary Cross Entropy**\n",
    "$$ Loss = -\\frac{1}{N}\\Sigma{y_i\\cdot log(p(y_i)) + (1-y_i)\\cdot log(1-p(y_i))} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 500 / 10000 (0%) \t Loss : nan\n",
      "EPOCH : 1000 / 10000 (0%) \t Loss : nan\n",
      "EPOCH : 1500 / 10000 (0%) \t Loss : nan\n",
      "EPOCH : 2000 / 10000 (0%) \t Loss : nan\n",
      "EPOCH : 2500 / 10000 (0%) \t Loss : nan\n",
      "EPOCH : 3000 / 10000 (0%) \t Loss : nan\n",
      "EPOCH : 3500 / 10000 (0%) \t Loss : nan\n",
      "EPOCH : 4000 / 10000 (0%) \t Loss : nan\n",
      "EPOCH : 4500 / 10000 (0%) \t Loss : nan\n",
      "EPOCH : 5000 / 10000 (0%) \t Loss : nan\n",
      "EPOCH : 5500 / 10000 (1%) \t Loss : nan\n",
      "EPOCH : 6000 / 10000 (1%) \t Loss : nan\n",
      "EPOCH : 6500 / 10000 (1%) \t Loss : nan\n",
      "EPOCH : 7000 / 10000 (1%) \t Loss : nan\n",
      "EPOCH : 7500 / 10000 (1%) \t Loss : nan\n",
      "EPOCH : 8000 / 10000 (1%) \t Loss : nan\n",
      "EPOCH : 8500 / 10000 (1%) \t Loss : nan\n",
      "EPOCH : 9000 / 10000 (1%) \t Loss : nan\n",
      "EPOCH : 9500 / 10000 (1%) \t Loss : nan\n",
      "EPOCH : 10000 / 10000 (1%) \t Loss : nan\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 10000\n",
    "lr = 0.01\n",
    "for epoch in range(1,EPOCH+1):\n",
    "    # 순전파 \n",
    "    l1 = torch.add(torch.matmul(X,w1),b1) # w1x + b1\n",
    "    a1 = sigmoid(l1)\n",
    "    l2 = torch.add(torch.matmul(a1,w2),b2)\n",
    "    y_pred = sigmoid(l2)\n",
    "    \n",
    "    loss = CE(y,y_pred)\n",
    "    \n",
    "    #역전파\n",
    "    # layer 2\n",
    "    d_loss = (y/y_pred) - (1-y)/(1-y_pred)\n",
    "    d_l2 = sigmoid_p(l2) * d_loss\n",
    "    d_b2 = d_l2\n",
    "    d_w2 = torch.matmul(torch.transpose(a1, 0, 1), d_l2) # 0,1 -> 행과 열 변경\n",
    "    \n",
    "    # layer 1\n",
    "    d_a1 = torch.matmul(d_l2, torch.transpose(w2, 0, 1))\n",
    "    d_l1 = sigmoid_p(l1) * d_a1\n",
    "    d_b1 = d_l1\n",
    "    d_w1 = torch.matmul(torch.transpose(X, 0, 1), d_l1)\n",
    "    \n",
    "    # gradient descent\n",
    "    w1 -= lr * d_w1\n",
    "    b1 -= lr * torch.mean(d_b1,0)\n",
    "    w2 -= lr * d_w2\n",
    "    b2 -= lr * torch.mean(d_b2,0)\n",
    "    \n",
    "    # result print\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"EPOCH : {epoch} / 10000 ({epoch/10000:.0f}%) \\t Loss : {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "역전파시에 행렬 연산이 이해가 안됨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
