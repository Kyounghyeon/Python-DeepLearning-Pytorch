{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chap01 - Autograd.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZoTer7yA2Jx"
      },
      "source": [
        "# 0. 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPEzxzWA8vzh",
        "outputId": "26cbe956-0e10-4cd6-91c8-a8db43a1c569"
      },
      "source": [
        "# 드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pv8ZWxil9ihv",
        "outputId": "d12fde44-2888-4b54-d8b4-c6966f163c75"
      },
      "source": [
        "cd /content/drive/MyDrive/colab"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/colab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aMYfSLyR-M0d",
        "outputId": "d52b6fe9-1253-4cc4-caa5-fe40656af41b"
      },
      "source": [
        "pwd"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/colab'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VDIQqt0A0Wk"
      },
      "source": [
        "# 1. 텐서"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwrJ3z6W-ttt"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV1Nu4Ik-1Cy"
      },
      "source": [
        "# 벡터 정의\n",
        "vector1 = torch.tensor([1., 2., 3.])\n",
        "vector2 = torch.tensor([4., 5., 6.])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwNXc1UM_PAE",
        "outputId": "7e3a5374-7322-4f2e-949c-7eb7dabca1a1"
      },
      "source": [
        "# torch 내장함수를 이용한 벡터wise 연산\n",
        "torch.add(vector1, vector2) # 같은 위치끼리 더하고\n",
        "torch.sub(vector1, vector2) # 빼고\n",
        "torch.mul(vector1, vector2) # 곱하고\n",
        "torch.div(vector1, vector2) # 나누고"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.2500, 0.4000, 0.5000])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxB8PLhJ_pDk"
      },
      "source": [
        "# 행렬 정의\n",
        "matrix1 = torch.tensor([[1.,2.], [3.,4.]])\n",
        "matrix2 = torch.tensor([[5.,6.], [7.,8.]])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-S8p8PyZ_4bs",
        "outputId": "d19de77b-de87-4d11-cbba-aba463a6bd4e"
      },
      "source": [
        "torch.add(matrix1 , matrix2) # 같은 위치끼리 더하고\n",
        "torch.sub(matrix1 , matrix2) # 빼고\n",
        "torch.mul(matrix1 , matrix2) # 곱하고\n",
        "torch.div(matrix1 , matrix2) # 나누고\n",
        "torch.matmul(matrix1, matrix2) # 행렬곱"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[19., 22.],\n",
              "        [43., 50.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtjx1cs_AKOX",
        "outputId": "33c980eb-eafd-4786-f989-ec630aa249fe"
      },
      "source": [
        "# 텐서 정의\n",
        "tensor1 = torch.tensor([ [[1.,2.], [3.,4.]],\n",
        "                         [[5.,6.], [7.,8.]] ])\n",
        "print(tensor1)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[1., 2.],\n",
            "         [3., 4.]],\n",
            "\n",
            "        [[5., 6.],\n",
            "         [7., 8.]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPzl3zR-P0jv",
        "outputId": "582b45a0-93c5-462c-cd1a-e13907c2e823"
      },
      "source": [
        "# 텐서 연산\n",
        "torch.matmul(tensor1, tensor1)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[  7.,  10.],\n",
              "         [ 15.,  22.]],\n",
              "\n",
              "        [[ 67.,  78.],\n",
              "         [ 91., 106.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VquRAUrAhYH"
      },
      "source": [
        "# 2. Autograd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj76R0FEA8D6"
      },
      "source": [
        "Back propagation(역전파)를 이용해서 파라미터를 업데이트할 때 Autograd 방식으로 쉽게 구현할 수 있음. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOkEDTiQBHP_"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  DEVICE = torch.device(\"cuda\")\n",
        "else:\n",
        "  DEVICE = torch.device(\"cpu\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6knuspiBcKA"
      },
      "source": [
        "`batch_size` : 딥러닝 모델에서 파라미터를 업데이트할 때 계산되는 데이터의 개수  \n",
        "`input_size` : 입력층의 노드 수  \n",
        "- `input_size`의 크기를 가지는 데이터를 `batch_size`만큼 사용  (아래 예제에서는 (64, 1000))  \n",
        "`hidden_size` : 은닉층의 노드 수   \n",
        "`output_size` : 출력되는 벡터 크기  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMIeNHOdBR8p"
      },
      "source": [
        "batch_size = 64 \n",
        "input_size = 1000\n",
        "hidden_size = 100\n",
        "output_size = 10"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWS70TV0Ch8o"
      },
      "source": [
        "# sample data 생성\n",
        "# 입력층\n",
        "x = torch.randn(batch_size,\n",
        "                input_size,\n",
        "                device = DEVICE,\n",
        "                dtype = torch.float,\n",
        "                requires_grad = False)\n",
        "\n",
        "# 은닉층1, 역전파를 통해 업데이트 해야 하는 대상\n",
        "w1 = torch.randn(input_size,\n",
        "                hidden_size,\n",
        "                device = DEVICE,\n",
        "                dtype = torch.float,\n",
        "                requires_grad = True)\n",
        "\n",
        "# 은닉층2, 역전파를 통해 업데이트 해야 하는 대상\n",
        "w2 = torch.randn(hidden_size,\n",
        "                output_size,\n",
        "                device = DEVICE,\n",
        "                dtype = torch.float,\n",
        "                requires_grad = True)\n",
        "\n",
        "# 출력층\n",
        "y = torch.randn(batch_size,\n",
        "                output_size,\n",
        "                device = DEVICE,\n",
        "                dtype = torch.float,\n",
        "                requires_grad = False)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Htwnj3XrEDol",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc04353e-0f21-4a08-c9cb-003cfd3c6159"
      },
      "source": [
        "lr = 1e-6 # learning rate\n",
        "for t in range(1, 501): # epoch = 500\n",
        "  y_pred = x.mm(w1).clamp(min = 0).mm(w2) # 예측값 = x와 w1 행렬곱 -> clamp(min=0 이므로, ReLU와 같은 활성화 함수)적용 -> w2 행렬곱 \n",
        "  \n",
        "  loss = (y_pred-y).pow(2).sum() # MSE\n",
        "  if t % 100 == 0:\n",
        "    print(f\"Iteration: {t} \\t Loss: {loss.item()}\")\n",
        "  loss.backward() # 각 파라미터 값에 대해 gradient를 계산 후 역전파 진행\n",
        "\n",
        "  with torch.no_grad():\n",
        "    # gradient 업데이트 \n",
        "    w1 -= lr * w1.grad \n",
        "    w2 -= lr * w2.grad\n",
        "    \n",
        "    # gradient 초기화\n",
        "    w1.grad.zero_()\n",
        "    w2.grad.zero_()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 100 \t Loss: 422.0916442871094\n",
            "Iteration: 200 \t Loss: 1.1017183065414429\n",
            "Iteration: 300 \t Loss: 0.0055623301304876804\n",
            "Iteration: 400 \t Loss: 0.00015774674830026925\n",
            "Iteration: 500 \t Loss: 3.206980545655824e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU2MdVM6FHwS"
      },
      "source": [
        "## clamp 함수 설명\n",
        "위 예시에서는 min = 0 이고, max는 없기 때문에, ReLU 함수와 똑같다.\n",
        "\n",
        "\\begin{cases}\n",
        "min,\\;if\\;x<min\\\\\n",
        "x,\\;if\\;min\\leq x\\leq\\max\\\\\n",
        "max, \\; if\\; x > max\n",
        "\\end{cases}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPh9rqaSIGUB"
      },
      "source": [
        "# 추가로 공부해볼 것들\n",
        "1. Back Propagation (역전파)\n",
        "2. Gradient Descent Method"
      ]
    }
  ]
}